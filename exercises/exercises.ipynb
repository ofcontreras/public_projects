{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_houses = 1000\n",
    "df=pd.DataFrame(columns=['id','closing_price', 'agent_1', 'agent_2', 'agent_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>closing_price</th>\n",
       "      <th>agent_1</th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.913723e+05</td>\n",
       "      <td>1.527132e+06</td>\n",
       "      <td>1.561870e+06</td>\n",
       "      <td>1.679284e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.997428e+06</td>\n",
       "      <td>4.288292e+05</td>\n",
       "      <td>1.839622e+06</td>\n",
       "      <td>1.479262e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.137832e+06</td>\n",
       "      <td>8.527096e+05</td>\n",
       "      <td>5.399991e+05</td>\n",
       "      <td>1.920775e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.192087e+06</td>\n",
       "      <td>7.059543e+05</td>\n",
       "      <td>7.786834e+05</td>\n",
       "      <td>1.994901e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.053307e+05</td>\n",
       "      <td>2.658393e+05</td>\n",
       "      <td>2.758467e+05</td>\n",
       "      <td>9.841670e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1.380208e+06</td>\n",
       "      <td>1.978761e+06</td>\n",
       "      <td>1.277041e+06</td>\n",
       "      <td>1.708322e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>8.144941e+05</td>\n",
       "      <td>1.951244e+06</td>\n",
       "      <td>6.434645e+05</td>\n",
       "      <td>1.602027e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>1.632120e+06</td>\n",
       "      <td>1.240640e+06</td>\n",
       "      <td>6.689857e+05</td>\n",
       "      <td>2.372524e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1.264569e+06</td>\n",
       "      <td>5.083779e+05</td>\n",
       "      <td>2.298767e+05</td>\n",
       "      <td>1.295094e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1.021466e+05</td>\n",
       "      <td>2.854251e+05</td>\n",
       "      <td>1.653342e+06</td>\n",
       "      <td>6.817316e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  closing_price       agent_1       agent_2       agent_3\n",
       "0      0   9.913723e+05  1.527132e+06  1.561870e+06  1.679284e+05\n",
       "1      1   1.997428e+06  4.288292e+05  1.839622e+06  1.479262e+05\n",
       "2      2   1.137832e+06  8.527096e+05  5.399991e+05  1.920775e+06\n",
       "3      3   1.192087e+06  7.059543e+05  7.786834e+05  1.994901e+06\n",
       "4      4   2.053307e+05  2.658393e+05  2.758467e+05  9.841670e+05\n",
       "..   ...            ...           ...           ...           ...\n",
       "995  995   1.380208e+06  1.978761e+06  1.277041e+06  1.708322e+06\n",
       "996  996   8.144941e+05  1.951244e+06  6.434645e+05  1.602027e+06\n",
       "997  997   1.632120e+06  1.240640e+06  6.689857e+05  2.372524e+05\n",
       "998  998   1.264569e+06  5.083779e+05  2.298767e+05  1.295094e+06\n",
       "999  999   1.021466e+05  2.854251e+05  1.653342e+06  6.817316e+05\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_houses = 1000\n",
    "df=pd.DataFrame()\n",
    "df[\"id\"] = range(n_houses)\n",
    "df[['closing_price', 'agent_1', 'agent_2', 'agent_3']]= np.random.uniform(80000,2000000,(n_houses,4))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>agent_1</th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_3</th>\n",
       "      <th>closing_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.527132e+06</td>\n",
       "      <td>1.561870e+06</td>\n",
       "      <td>1.679284e+05</td>\n",
       "      <td>9.913723e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.288292e+05</td>\n",
       "      <td>1.839622e+06</td>\n",
       "      <td>1.479262e+05</td>\n",
       "      <td>1.997428e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.527096e+05</td>\n",
       "      <td>5.399991e+05</td>\n",
       "      <td>1.920775e+06</td>\n",
       "      <td>1.137832e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.059543e+05</td>\n",
       "      <td>7.786834e+05</td>\n",
       "      <td>1.994901e+06</td>\n",
       "      <td>1.192087e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.658393e+05</td>\n",
       "      <td>2.758467e+05</td>\n",
       "      <td>9.841670e+05</td>\n",
       "      <td>2.053307e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1.978761e+06</td>\n",
       "      <td>1.277041e+06</td>\n",
       "      <td>1.708322e+06</td>\n",
       "      <td>1.380208e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>1.951244e+06</td>\n",
       "      <td>6.434645e+05</td>\n",
       "      <td>1.602027e+06</td>\n",
       "      <td>8.144941e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>1.240640e+06</td>\n",
       "      <td>6.689857e+05</td>\n",
       "      <td>2.372524e+05</td>\n",
       "      <td>1.632120e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>5.083779e+05</td>\n",
       "      <td>2.298767e+05</td>\n",
       "      <td>1.295094e+06</td>\n",
       "      <td>1.264569e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>2.854251e+05</td>\n",
       "      <td>1.653342e+06</td>\n",
       "      <td>6.817316e+05</td>\n",
       "      <td>1.021466e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       agent_1       agent_2       agent_3  closing_price\n",
       "0      0  1.527132e+06  1.561870e+06  1.679284e+05   9.913723e+05\n",
       "1      1  4.288292e+05  1.839622e+06  1.479262e+05   1.997428e+06\n",
       "2      2  8.527096e+05  5.399991e+05  1.920775e+06   1.137832e+06\n",
       "3      3  7.059543e+05  7.786834e+05  1.994901e+06   1.192087e+06\n",
       "4      4  2.658393e+05  2.758467e+05  9.841670e+05   2.053307e+05\n",
       "..   ...           ...           ...           ...            ...\n",
       "995  995  1.978761e+06  1.277041e+06  1.708322e+06   1.380208e+06\n",
       "996  996  1.951244e+06  6.434645e+05  1.602027e+06   8.144941e+05\n",
       "997  997  1.240640e+06  6.689857e+05  2.372524e+05   1.632120e+06\n",
       "998  998  5.083779e+05  2.298767e+05  1.295094e+06   1.264569e+06\n",
       "999  999  2.854251e+05  1.653342e+06  6.817316e+05   1.021466e+05\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long = df.melt(id_vars=[\"id\"], value_vars=['closing_price','agent_1', 'agent_2', 'agent_3'], var_name=\"agent\", value_name=\"valuation\")\n",
    "df_long\n",
    "df_wide = df_long.pivot(index=\"id\", columns=\"agent\",values=\"valuation\")\n",
    "df_wide.columns.name=None\n",
    "df_wide.reset_index(inplace=True)\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3000 entries, 1000 to 3999\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             3000 non-null   int64  \n",
      " 1   agent          3000 non-null   object \n",
      " 2   valuation      3000 non-null   float64\n",
      " 3   closing_price  3000 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 117.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_calculations = df_long.merge(right=df[[\"id\",\"closing_price\"]], how=\"left\", on=\"id\")\n",
    "df_calculations = df_calculations[df_calculations['agent']!='closing_price']\n",
    "df_calculations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent_1</td>\n",
       "      <td>770992.359272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agent_2</td>\n",
       "      <td>761946.284385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent_3</td>\n",
       "      <td>791080.702413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     agent           rmse\n",
       "0  agent_1  770992.359272\n",
       "1  agent_2  761946.284385\n",
       "2  agent_3  791080.702413"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calculations.reset_index(inplace=True,drop=True)\n",
    "df_calculations['se'] = (df_calculations['closing_price']-df_calculations['valuation'])**2\n",
    "rmse=df_calculations.groupby(by=['agent']).agg({\"se\":[np.mean]})**0.5\n",
    "rmse.rename(columns={\"se\":\"rmse\"}, level=0, inplace=True)\n",
    "rmse.droplevel(level=1,axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>agent</th>\n",
       "      <th>valuation</th>\n",
       "      <th>closing_price</th>\n",
       "      <th>se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>733</td>\n",
       "      <td>agent_1</td>\n",
       "      <td>1.515630e+06</td>\n",
       "      <td>1.577846e+06</td>\n",
       "      <td>3.870852e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>169</td>\n",
       "      <td>agent_2</td>\n",
       "      <td>1.168701e+06</td>\n",
       "      <td>1.227834e+06</td>\n",
       "      <td>3.496715e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>764</td>\n",
       "      <td>agent_2</td>\n",
       "      <td>9.476392e+05</td>\n",
       "      <td>1.088463e+06</td>\n",
       "      <td>1.983144e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>925</td>\n",
       "      <td>agent_3</td>\n",
       "      <td>1.177496e+06</td>\n",
       "      <td>1.115106e+06</td>\n",
       "      <td>3.892454e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>946</td>\n",
       "      <td>agent_3</td>\n",
       "      <td>1.540086e+06</td>\n",
       "      <td>1.558491e+05</td>\n",
       "      <td>1.916111e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>800</td>\n",
       "      <td>agent_1</td>\n",
       "      <td>1.701244e+06</td>\n",
       "      <td>8.592450e+05</td>\n",
       "      <td>7.089628e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>37</td>\n",
       "      <td>agent_3</td>\n",
       "      <td>1.503174e+06</td>\n",
       "      <td>1.225842e+06</td>\n",
       "      <td>7.691318e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>380</td>\n",
       "      <td>agent_3</td>\n",
       "      <td>9.925072e+05</td>\n",
       "      <td>5.761838e+05</td>\n",
       "      <td>1.733252e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>51</td>\n",
       "      <td>agent_2</td>\n",
       "      <td>1.367531e+06</td>\n",
       "      <td>1.894708e+06</td>\n",
       "      <td>2.779146e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>agent_1</td>\n",
       "      <td>4.537934e+05</td>\n",
       "      <td>1.593820e+06</td>\n",
       "      <td>1.299660e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    agent     valuation  closing_price            se\n",
       "733   733  agent_1  1.515630e+06   1.577846e+06  3.870852e+09\n",
       "1169  169  agent_2  1.168701e+06   1.227834e+06  3.496715e+09\n",
       "1764  764  agent_2  9.476392e+05   1.088463e+06  1.983144e+10\n",
       "2925  925  agent_3  1.177496e+06   1.115106e+06  3.892454e+09\n",
       "2946  946  agent_3  1.540086e+06   1.558491e+05  1.916111e+12\n",
       "...   ...      ...           ...            ...           ...\n",
       "800   800  agent_1  1.701244e+06   8.592450e+05  7.089628e+11\n",
       "2037   37  agent_3  1.503174e+06   1.225842e+06  7.691318e+10\n",
       "2380  380  agent_3  9.925072e+05   5.761838e+05  1.733252e+11\n",
       "1051   51  agent_2  1.367531e+06   1.894708e+06  2.779146e+11\n",
       "149   149  agent_1  4.537934e+05   1.593820e+06  1.299660e+12\n",
       "\n",
       "[2400 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_calculations.sample(frac=0.8,axis=0, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(seed=123)\n",
    "\n",
    "mask = rng.choice(df_calculations.index, size=int(round(0.8*df_calculations.shape[0],0)), replace=False)\n",
    "df_calculations.loc[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      9.913723e+05\n",
       "1      1.997428e+06\n",
       "2      1.137832e+06\n",
       "3      1.192087e+06\n",
       "4      2.053307e+05\n",
       "           ...     \n",
       "995    1.380208e+06\n",
       "996    8.144941e+05\n",
       "997    1.632120e+06\n",
       "998    1.264569e+06\n",
       "999    1.021466e+05\n",
       "Name: closing_price, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['closing_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5.357597e+05\n",
       "1     -1.568598e+06\n",
       "2     -2.851219e+05\n",
       "3     -4.861327e+05\n",
       "4      6.050861e+04\n",
       "           ...     \n",
       "995    5.985529e+05\n",
       "996    1.136750e+06\n",
       "997   -3.914797e+05\n",
       "998   -7.561913e+05\n",
       "999    1.832786e+05\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x['agent_1']-x['closing_price'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>closing_price</th>\n",
       "      <th>agent_1</th>\n",
       "      <th>agent_2</th>\n",
       "      <th>agent_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-499.5</td>\n",
       "      <td>-46877.375708</td>\n",
       "      <td>507707.054615</td>\n",
       "      <td>556798.255416</td>\n",
       "      <td>-876770.915620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-498.5</td>\n",
       "      <td>959177.966717</td>\n",
       "      <td>-590595.759990</td>\n",
       "      <td>834550.266739</td>\n",
       "      <td>-896773.126562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-497.5</td>\n",
       "      <td>99581.872713</td>\n",
       "      <td>-166715.301196</td>\n",
       "      <td>-465072.591479</td>\n",
       "      <td>876076.051914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-496.5</td>\n",
       "      <td>153837.407966</td>\n",
       "      <td>-313470.596343</td>\n",
       "      <td>-226388.263745</td>\n",
       "      <td>950201.588063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-495.5</td>\n",
       "      <td>-832918.940678</td>\n",
       "      <td>-753585.620614</td>\n",
       "      <td>-729225.017806</td>\n",
       "      <td>-60532.233009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>495.5</td>\n",
       "      <td>341958.581265</td>\n",
       "      <td>959336.243892</td>\n",
       "      <td>271968.884358</td>\n",
       "      <td>663622.634682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>496.5</td>\n",
       "      <td>-223755.533282</td>\n",
       "      <td>931819.561248</td>\n",
       "      <td>-361607.245551</td>\n",
       "      <td>557327.523424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>497.5</td>\n",
       "      <td>593870.017217</td>\n",
       "      <td>221215.062575</td>\n",
       "      <td>-336086.031646</td>\n",
       "      <td>-807446.879634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>498.5</td>\n",
       "      <td>226319.629067</td>\n",
       "      <td>-511046.976962</td>\n",
       "      <td>-775195.052353</td>\n",
       "      <td>250394.234439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>499.5</td>\n",
       "      <td>-936103.046096</td>\n",
       "      <td>-733999.769475</td>\n",
       "      <td>648269.944537</td>\n",
       "      <td>-362967.724984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  closing_price        agent_1        agent_2        agent_3\n",
       "0   -499.5  -46877.375708  507707.054615  556798.255416 -876770.915620\n",
       "1   -498.5  959177.966717 -590595.759990  834550.266739 -896773.126562\n",
       "2   -497.5   99581.872713 -166715.301196 -465072.591479  876076.051914\n",
       "3   -496.5  153837.407966 -313470.596343 -226388.263745  950201.588063\n",
       "4   -495.5 -832918.940678 -753585.620614 -729225.017806  -60532.233009\n",
       "..     ...            ...            ...            ...            ...\n",
       "995  495.5  341958.581265  959336.243892  271968.884358  663622.634682\n",
       "996  496.5 -223755.533282  931819.561248 -361607.245551  557327.523424\n",
       "997  497.5  593870.017217  221215.062575 -336086.031646 -807446.879634\n",
       "998  498.5  226319.629067 -511046.976962 -775195.052353  250394.234439\n",
       "999  499.5 -936103.046096 -733999.769475  648269.944537 -362967.724984\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transform(func=lambda x: x-x.mean(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'EmployeeName': ['Callen Dunkley', 'Sarah Rayner', 'Jeanette Sloan', 'Kaycee Acosta', 'Henri Conroy', 'Emma Peralta', 'Martin Butt', 'Alex Jensen', 'Kim Howarth', 'Jane Burnett'],\n",
    "                    'Department': ['Accounting', 'Engineering', 'Engineering', 'HR', 'HR', 'HR', 'Data Science', 'Data Science', 'Accounting', 'Data Science'],\n",
    "                    'HireDate': [2010, 2018, 2012, 2014, 2014, 2018, 2020, 2018, 2020, 2012],\n",
    "                    'Sex': ['M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F'],\n",
    "                    'Birthdate': ['04/09/1982', '14/04/1981', '06/05/1997', '08/01/1986', '10/10/1988', '12/11/1992', '10/04/1991', '16/07/1995', '08/10/1992', '11/10/1979'],\n",
    "                    'Weight': [78, 80, 66, 67, 90, 57, 115, 87, 95, 57],\n",
    "                    'Height': [176, 160, 169, 157, 185, 164, 195, 180, 174, 165],\n",
    "                    'Kids': [2, 1, 0, 1, 1, 0, 2, 0, 3, 1],\n",
    "                     'mobile' : ['+1 224 388 1527','224-388-1527','1 2243881527','12243881527','224388 1527','+2243881527','+2243881527',\n",
    "                                 '$1-224 388 1527','1334 388 1527','224$388 1527']\n",
    "                    })\n",
    "data.to_csv('./df_employees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_cal(Birthdate):\n",
    "  \n",
    "  today = pd.to_datetime('today')\n",
    "  age = today.year - Birthdate.year - ( (today.month, today.day) < (Birthdate.month, Birthdate.day) )\n",
    "  return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Interface used to load a :class:`DataFrame` from external storage systems\n",
      "    (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      "    to access this.\n",
      "\n",
      "    .. versionadded:: 1.4\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dir(spark.read)    # to see an object's methods\n",
    "print(spark.read.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- EmployeeName: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- HireDate: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Kids: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n",
      "+---+--------------+-----------+--------+---+----------+------+------+----+---------------+\n",
      "|_c0|  EmployeeName| Department|HireDate|Sex| Birthdate|Weight|Height|Kids|         mobile|\n",
      "+---+--------------+-----------+--------+---+----------+------+------+----+---------------+\n",
      "|  0|Callen Dunkley| Accounting|    2010|  M|04/09/1982|    78|   176|   2|+1 224 388 1527|\n",
      "|  1|  Sarah Rayner|Engineering|    2018|  F|14/04/1981|    80|   160|   1|   224-388-1527|\n",
      "|  2|Jeanette Sloan|Engineering|    2012|  F|06/05/1997|    66|   169|   0|   1 2243881527|\n",
      "|  3| Kaycee Acosta|         HR|    2014|  F|08/01/1986|    67|   157|   1|    12243881527|\n",
      "|  4|  Henri Conroy|         HR|    2014|  M|10/10/1988|    90|   185|   1|    224388 1527|\n",
      "+---+--------------+-----------+--------+---+----------+------+------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "employees = spark.read.csv('./df_employees.csv', header=True)\n",
    "employees.printSchema()\n",
    "employees.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o69.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\Users\\oscar\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\Users\\oscar\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:608)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[39m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39m./df_employees.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[39m.\u001b[39mselect(F\u001b[39m.\u001b[39msplit(F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mEmployeeName\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m.\u001b[39mcount()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39m#results.show(n=20)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#results.write.csv('./results_single_partition.csv')\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m results\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39m./data/results_single_partition.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# Writing our results under a single partition\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Creating our own simple SparkSession for batch submit\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom pyspark.sql import SparkSession\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mspark-submit ./code/Ch02/word_count_submit.py\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\miniconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:955\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression, sep\u001b[39m=\u001b[39msep, quote\u001b[39m=\u001b[39mquote, escape\u001b[39m=\u001b[39mescape, header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m    949\u001b[0m                nullValue\u001b[39m=\u001b[39mnullValue, escapeQuotes\u001b[39m=\u001b[39mescapeQuotes, quoteAll\u001b[39m=\u001b[39mquoteAll,\n\u001b[0;32m    950\u001b[0m                dateFormat\u001b[39m=\u001b[39mdateFormat, timestampFormat\u001b[39m=\u001b[39mtimestampFormat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    953\u001b[0m                charToEscapeQuoteEscaping\u001b[39m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[0;32m    954\u001b[0m                encoding\u001b[39m=\u001b[39mencoding, emptyValue\u001b[39m=\u001b[39memptyValue, lineSep\u001b[39m=\u001b[39mlineSep)\n\u001b[1;32m--> 955\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\oscar\\miniconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\miniconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o69.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\Users\\oscar\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\Users\\oscar\\spark\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:608)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "results = (\n",
    "spark.read.csv(\"./df_employees.csv\", header=True)\n",
    ".select(F.split(F.col(\"EmployeeName\"), \" \").alias(\"line\"))\n",
    ".select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    ".select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    ".select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    ".where(F.col(\"word\") != \"\")\n",
    ".groupby(\"word\")\n",
    ".count()\n",
    ")\n",
    "#results.show(n=20)\n",
    "#results.write.csv('./results_single_partition.csv')\n",
    "results.coalesce(1).write.csv('./data/results_single_partition.csv')  # Writing our results under a single partition\n",
    "\n",
    "'''\n",
    "# Creating our own simple SparkSession for batch submit\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    ".appName(\"Counting word occurences from a book.\")\n",
    ".getOrCreate())\n",
    "\n",
    "spark-submit ./code/Ch02/word_count_submit.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grocery_list = [\n",
    "[\"Banana\", 2, 1.74],\n",
    "[\"Apple\", 4, 2.04],\n",
    "[\"Carrot\", 1, 1.09],\n",
    "[\"Cake\", 1, 10.99],\n",
    "]\n",
    "df_grocery_list = spark.createDataFrame(my_grocery_list, [\"Item\", \"Quantity\", \"Price\"])\n",
    "df_grocery_list.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|EmployeeName  |\n",
      "+--------------+\n",
      "|Callen Dunkley|\n",
      "|Sarah Rayner  |\n",
      "|Jeanette Sloan|\n",
      "|Kaycee Acosta |\n",
      "|Henri Conroy  |\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+------------+--------+---+----------+------+------+----+---------------+\n",
      "|_c0|  Department|HireDate|Sex| Birthdate|Weight|Height|Kids|         mobile|\n",
      "+---+------------+--------+---+----------+------+------+----+---------------+\n",
      "|  0|  Accounting|    2010|  M|04/09/1982|    78|   176|   2|+1 224 388 1527|\n",
      "|  1| Engineering|    2018|  F|14/04/1981|    80|   160|   1|   224-388-1527|\n",
      "|  2| Engineering|    2012|  F|06/05/1997|    66|   169|   0|   1 2243881527|\n",
      "|  3|          HR|    2014|  F|08/01/1986|    67|   157|   1|    12243881527|\n",
      "|  4|          HR|    2014|  M|10/10/1988|    90|   185|   1|    224388 1527|\n",
      "|  5|          HR|    2018|  F|12/11/1992|    57|   164|   0|    +2243881527|\n",
      "|  6|Data Science|    2020|  M|10/04/1991|   115|   195|   2|    +2243881527|\n",
      "|  7|Data Science|    2018|  M|16/07/1995|    87|   180|   0|$1-224 388 1527|\n",
      "|  8|  Accounting|    2020|  M|08/10/1992|    95|   174|   3|  1334 388 1527|\n",
      "|  9|Data Science|    2012|  F|11/10/1979|    57|   165|   1|   224$388 1527|\n",
      "+---+------------+--------+---+----------+------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\"EmployeeName\").show(5, False)\n",
    "employees2 = employees.drop(\"EmployeeName\")\n",
    "employees2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+---+\n",
      "|_c0|  EmployeeName|  Department|HireDate|Sex| Birthdate|Weight|Height|Kids|         mobile|day|\n",
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+---+\n",
      "|  0|Callen Dunkley|  Accounting|    2010|  M|04/09/1982|    78|   176|   2|+1 224 388 1527|  4|\n",
      "|  1|  Sarah Rayner| Engineering|    2018|  F|14/04/1981|    80|   160|   1|   224-388-1527| 14|\n",
      "|  2|Jeanette Sloan| Engineering|    2012|  F|06/05/1997|    66|   169|   0|   1 2243881527|  6|\n",
      "|  3| Kaycee Acosta|          HR|    2014|  F|08/01/1986|    67|   157|   1|    12243881527|  8|\n",
      "|  4|  Henri Conroy|          HR|    2014|  M|10/10/1988|    90|   185|   1|    224388 1527| 10|\n",
      "|  5|  Emma Peralta|          HR|    2018|  F|12/11/1992|    57|   164|   0|    +2243881527| 12|\n",
      "|  6|   Martin Butt|Data Science|    2020|  M|10/04/1991|   115|   195|   2|    +2243881527| 10|\n",
      "|  7|   Alex Jensen|Data Science|    2018|  M|16/07/1995|    87|   180|   0|$1-224 388 1527| 16|\n",
      "|  8|   Kim Howarth|  Accounting|    2020|  M|08/10/1992|    95|   174|   3|  1334 388 1527|  8|\n",
      "|  9|  Jane Burnett|Data Science|    2012|  F|11/10/1979|    57|   165|   1|   224$388 1527| 11|\n",
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.select(\n",
    "    \"Birthdate\",\n",
    "    F.col(\"Birthdate\").substr(1, 2).cast(\"int\").alias(\"day\"),\n",
    "    F.col(\"Birthdate\").substr(4, 2).cast(\"int\").alias(\"month\"),\n",
    "    F.col(\"Birthdate\").substr(7, 4).cast(\"int\").alias(\"year\"),\n",
    "    ).distinct().show()\n",
    "\n",
    "employees.withColumn(\"day\",F.col(\"Birthdate\").substr(1, 2).cast(\"int\")).show() # another way to create a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+\n",
      "|_c0|  EmployeeName|  Department|HireDate|Sex| Birthdate|Weight|Height|Kids|         mobile|\n",
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+\n",
      "|  0|Callen Dunkley|  Accounting|    2010|  M|04/09/1982|    78|   176|   2|+1 224 388 1527|\n",
      "|  1|  Sarah Rayner| Engineering|    2018|  F|14/04/1981|    80|   160|   1|   224-388-1527|\n",
      "|  2|Jeanette Sloan| Engineering|    2012|  F|06/05/1997|    66|   169|   0|   1 2243881527|\n",
      "|  3| Kaycee Acosta|          HR|    2014|  F|08/01/1986|    67|   157|   1|    12243881527|\n",
      "|  4|  Henri Conroy|          HR|    2014|  M|10/10/1988|    90|   185|   1|    224388 1527|\n",
      "|  5|  Emma Peralta|          HR|    2018|  F|12/11/1992|    57|   164|   0|    +2243881527|\n",
      "|  6|   Martin Butt|Data Science|    2020|  M|10/04/1991|   115|   195|   2|    +2243881527|\n",
      "|  7|   Alex Jensen|Data Science|    2018|  M|16/07/1995|    87|   180|   0|$1-224 388 1527|\n",
      "|  8|   Kim Howarth|  Accounting|    2020|  M|08/10/1992|    95|   174|   3|  1334 388 1527|\n",
      "|  9|  Jane Burnett|Data Science|    2012|  F|11/10/1979|    57|   165|   1|   224$388 1527|\n",
      "+---+--------------+------------+--------+---+----------+------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns \n",
    "#employees.withColumnRenamed(\"Birthdate\", \"birthdate\").printSchema()\n",
    "#employees.toDF(*[x.lower() for x in employees.columns]).printSchema()\n",
    "\n",
    "# select in alphabetical order\n",
    "#employees.select(sorted(employees.columns)).printSchema()\n",
    "\n",
    "## describe\n",
    "#employees.describe().show()\n",
    "#employees.describe(\"EmployeeName\").show()\n",
    "#for i in employees.columns:\n",
    "#    employees.describe(i).show()\n",
    "\n",
    "## summary\n",
    "#employees.summary().show()\n",
    "#employees.select(\"EmployeeName\").summary().show()\n",
    "#employees.select(\"EmployeeName\").summary(\"min\", \"10%\", \"90%\", \"max\").show()\n",
    "\n",
    "## joins\n",
    "#[LEFT].join(\n",
    "#[RIGHT],\n",
    "#on=[PREDICATES]\n",
    "#how=[METHOD]  )   left, right, inner, outer,\n",
    "#)\n",
    "\n",
    "## agg\n",
    "#employees.groupBy(\"Department\").count().show\n",
    "#employees.groupBy(\"Department\").agg(F.sum(\"Kids\").alias(\"total_kids\")).show()\n",
    "\n",
    "## other\n",
    "#employees.dropna(subset=[\"Department\"]).show()\n",
    "#employees.fillna(0).show()  # can use a dictionary as in python\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
